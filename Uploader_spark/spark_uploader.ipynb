{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b97951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = '/opt/cloudera/parcels/SPARK2/lib/spark2'\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/python/virtualenv/jupiter/lib'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/cloudera/parcels/PYENV.ZNO20008661/bin/python'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/cloudera/parcels/PYENV.ZNO20008661/bin/python'\n",
    "\n",
    "sys.parth.insert(0, os.path.join (spark_home, 'python'))\n",
    "sys.parth.insert(0, os.path.join (spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.sql.functions import udf, col, explode_outer, lower as Low, upper as Up, broadcast, concat_ws, when, lit, trim, substring\n",
    "from pyspark.sql.types import StringType, ArrayType, BoolenType\n",
    "\n",
    "from pyspark.ml.feature import Tokinizer\n",
    "\n",
    "import re \n",
    "import pandas as pd\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "conf = SparkConf().setAppName('jal_spark_uploader_1').setMaster('yarn')\n",
    "\n",
    "conf.setAll([\n",
    "    ('spark.driver.cores', '1'),\n",
    "    ('spark.driver.memory', '20g'),\n",
    "    ('spark.driver.maxResultSize', '8g'),\n",
    "    ('spark.yarn.executor.memoryOverhead', '2g'),\n",
    "    ('spark.port.maxRetries', '150'),\n",
    "    ('spark.executor.memory', '20g'),\n",
    "    ('spark.executor.cores', 4),\n",
    "    ('spark.executor.instances', 6),\n",
    "    ('spark.sql.shuffle.partitions', 200),\n",
    "    \n",
    "    ('spark.default.parallelism', 512),\n",
    "    ('spark.dynamicAllocation.enabled', 'false'),\n",
    "    ('spark.kryoserializer.buffer.max', '1g'),\n",
    "    ('spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation', 'true'),\n",
    "    ('spark.driver.allowMultipleContexts', 'true'),\n",
    "    ('spark.sql.hive.convertMetastoreParquet', 'false'),\n",
    "    ('mapred.input.dir.recursive', 'true'),\n",
    "    ('hive.mapred.supports.subdirectories', 'true'),\n",
    "    ('hive.optimize.listbucketing', 'true'),\n",
    "    ('spark.sql.hive.convertMetastoreParquet', 'false'),\n",
    "    ('hive.supports.subdirectories', 'true'),\n",
    "    ('hive.mapred.supports.subdirectories', 'true'),\n",
    "    ('mapred.input.dir.recursive', 'true'),\n",
    "    ('hive.auto.convert.join', 'false'),\n",
    "    ('hive.auto.convert.join.noconditionaltask', 'false')\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"SELECT\n",
    "t1.col1\n",
    "t1.col2\n",
    "FROM tabel1 as t1\n",
    "where 1=1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.write.mode('overwrite').saveAsTable('SandBox_db.table_name', format='parquet')\n",
    "# format='parquet'    'orc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.option('header', 'true').option('delimiter', '|').csv('jal_table_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.table(\"SandBox_db.table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e441e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.show(1, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c69733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a65123",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
